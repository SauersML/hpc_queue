# hpc_queue

Single CLI: `q`

- Local submits jobs.
- HPC runs compute worker.
- Local pulls results.

## Keys

- `api-key`: auth for public submit endpoint (`/jobs`), stored as `API_KEY`.
- `queue-token`: auth for Cloudflare Queue API (pull/ack/enqueue), stored as `CF_QUEUES_API_TOKEN`.

## Setup

Install `q` command:

```bash
curl -fsSL https://raw.githubusercontent.com/SauersML/hpc_queue/main/install.sh | bash
```

Run one command to configure:

```bash
q login
```

Update both local and HPC to latest `main` (commit-pinned install) in one command:

```bash
q update
```

`q update` behavior:
- resolves latest commit on `main`
- runs `curl .../<commit>/install.sh | bash` on local
- submits an HPC host update job with the same commit
- requests graceful HPC reload: finish in-flight jobs first, then restart worker

`q login` now only needs:
- `queue-token`
- `api-key`

`q grab` uses presigned URLs generated by the Worker (R2 secrets stay server-side).

## Local machine

Submit a command job:

```bash
q submit ls
q submit "python /work/script.py --iters 100"
q python3 /work/thisfile.py
```

Container jobs can read HPC host files through a built-in read-only portal:

```bash
q submit --wait "ls -lah /portal/users"
q submit --wait "ls -lah /portal/projects"
q submit --wait "cat /portal/etc/os-release"
```

Use `/portal/<absolute-host-path>` to reference host files from inside the container.

Submit a host command job (outside Apptainer on HPC node):

```bash
q host "nproc"
q host "top -n 1"
```

Run a local file on HPC (file is uploaded into the job payload):

```bash
q run-file ./script.py
q run-file ./script.py -- --arg1 10 --arg2 test
```

By default `q run-file` executes with `python`.
Set a different runner with `--runner`, for example:

```bash
q run-file --runner bash ./job.sh -- --fast
```

`q submit` now returns immediately by default.
It starts a background local results watcher and writes:
- `local-results/<job_id>.json`
- `local-results/<job_id>.stdout.log`
- `local-results/<job_id>.stderr.log`

Use blocking mode only when needed:

```bash
q submit --wait ls
```

Pull one batch of result messages:

```bash
q results
```

`q results` also writes `local-results/<job_id>.json` for completed jobs.

View job logs on demand:

```bash
q logs <job_id>
```

`q logs` reads local files when available, and otherwise falls back to cached queue result tails.

Grab one file from HPC to local current directory (private transfer):

```bash
q grab <job_id> file.png
q grab <job_id> /path/to/file.png
q grab host /path/to/host/file.png
```

Choose a local output path:

```bash
q grab <job_id> /path/to/file.png --output ./my-copy.png
q grab host /path/to/host/file.png -o ./downloads/
```

`q grab` behavior:
- Uses private R2 presigned URLs under the hood.
- Uploads from HPC (host mode or container file path mode), downloads to local.
- Deletes the temporary object from R2 after local file is written.

View one job's last known status (no queue polling):

```bash
q job <job_id>
```

`q job` reads only local artifacts/cache (`local-results` + `results_cache.jsonl`).

## CLI Reference

You can print full command help any time:

```bash
q --help
q submit --help
q host --help
q run-file --help
```

`q login`
- `--queue-token <token>`: set Cloudflare queue API token.
- `--api-key <key>`: set `/jobs` API key (auto-generated if omitted).

Worker admin one-time setup for `q grab`:
- Set Worker secrets in `producer-worker`:
  - `R2_ACCESS_KEY_ID`
  - `R2_SECRET_ACCESS_KEY`
  - optional: `R2_BUCKET` (default: `hpc-queue-grab`)
  - optional: `R2_ACCOUNT_ID` (default hardcoded CF account)
  - optional: `R2_REGION` (default: `auto`)
- Example:

```bash
cd producer-worker
npx wrangler secret put R2_ACCESS_KEY_ID
npx wrangler secret put R2_SECRET_ACCESS_KEY
npx wrangler deploy
```

`q submit <command...>`
- Runs command inside Apptainer runtime on HPC.
- Container mount points:
  - `/work` (job scratch and artifacts)
  - `/gnomon` (synced gnomon repo)
  - `/reagle` (synced reagle repo)
  - `/portal` (read-only host filesystem view)
- `--wait`: block until local result file exists, then print logs.

`q host <command...>`
- Runs command directly on HPC host (outside container).
- `--wait`: block until local result file exists, then print logs.

`q run-file [--runner <bin>] <local_file> [-- <args...>]`
- Uploads local file into job payload and executes it on HPC in container.
- Default runner is `python`.
- `--runner bash` for shell scripts, or `--runner ""` to execute directly.
- `--wait`: block until local result file exists, then print logs.

`q logs <job_id>`
- Prints job summary plus stdout/stderr from local artifacts or cache.

`q job <job_id>`
- Prints last known event/status for one job from local artifacts/cache only.
- Does not call Cloudflare queue APIs.

`q results`
- Pull one batch from results queue and write local artifacts.

`q status`
- Shows local process status.
- On local, also includes remote heartbeat fields:
  - `hpc_running_remote`
  - `hpc_last_heartbeat`
  - `hpc_heartbeat_age_seconds`
- Default output is human-readable text.
- Use `q status --json` for raw JSON output.

`q clear jobs|results|all`
- Purges messages from selected queue(s) using pull+ack loops.
- `--batch-size <n>`: pull size per cycle (default `100`).
- `--max-batches <n>`: max cycles (default `200`).

`q stop`
- Stops local worker/watcher processes.
- `--all`: also clears jobs + results queues.

`q update`
- Updates local and HPC installs to latest `main` commit.
- Uses commit-pinned install URL for reproducibility.
- Requests graceful HPC worker reload (drain running jobs, then restart).
- `--no-wait`: do not wait for remote host update job completion.

`q grab <target> <path> [-o|--output <local_path>]`
- `target=host`: path is resolved on HPC host filesystem.
- `target=<job_id>`: path is resolved inside container context for that job.
- Copies one file to local machine.
- Auto-deletes temporary R2 object after local write succeeds.

## HPC node

Start worker + auto image refresh in one command:

```bash
q start
```

If `q start` fails, verify `apptainer` is installed and GitHub release download is reachable.

Check status:

```bash
q status
```

`q status` on local also reports `hpc_running_remote` using heartbeat events from the HPC consumer.

Stop worker:

```bash
q stop
```

Stop local processes and clear both queues:

```bash
q stop --all
```

Clear queues directly:

```bash
q clear jobs
q clear results
q clear all
```

## How worker image updates happen

`q start` resolves the remote image digest first.
- If digest is unchanged, it skips download.
- If digest changed, it downloads the new `.sif` built by GitHub Actions and updates local image.

This gives both properties:
- never stale (digest checked every startup)
- no unnecessary re-pulls

The running HPC consumer also checks digest before each job execution, so long-running workers stay fresh without forced re-pulls.

## Runtime image contents

The default runtime image now includes:
- Python 3.12 + common data/biology Python packages from `containers/requirements.txt`
- Rust toolchain (`rustup` stable)
- Standard dev tools (`build-essential`, `git`, `curl`, `jq`, `vim`, `htop`, etc.)
- `bcftools`
- `plink2` and `gctb` (via micromamba/bioconda)
- Empty mount targets:
  - `/gnomon`
  - `/reagle`

## Repo freshness (`gnomon` / `reagle`)

For container jobs, the HPC consumer now syncs these repos before each job and bind-mounts them:
- `/gnomon`
- `/reagle`

This means jobs see latest refs (default `main`) without waiting for a container rebuild.
